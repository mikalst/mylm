--- 
title: "TMA4315: Compulsory exercise 1 (title)" 
subtitle: "Group 0: Henrik Syversveen Lie, Mikal Stapnes" 
date: "`r format(Sys.time(), '%d.%m.%Y')`" # the current date, can be regular text as well
output: # 3rd letter intentation hierarchy
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
  # pdf_document:
  #  toc: false
  #  toc_depth: 2

---

```{r setup, include = FALSE}
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Part 1

**Bold**

_italic_

To get a pdf file, make comments of the lines with the "html_document" information, and make the lines with the "pdf_document" information regular, and vice versa.

## a)

Your answer for part 1a)

```{r}
# some R code for part 1a)
library(ggplot2)
ggplot(data.frame(x = 1:10, Group = rep(c("A", "B"), 5)), aes(x = x, y = x, col = Group)) + geom_point() + theme_bw()
```


```{r, eval=T, echo=FALSE}
#install.packages("car")
library(car)
library(GGally)
data(SLID, package = "carData")
SLID = SLID[complete.cases(SLID), ]
ggpairs(SLID)
```

Draw a matrix of diagnostic plots of all the variables and comment briefly on the relationship between some of the variables. 

`Wage` has a positive correlation with the quantitative variables `education` and `age`. In addition we see that `sex`="Male" has a positive effect on `Wage`. Increasing `education`, `age` or switching `sex` to "Male" will, on average, result in an increase in `Wage` in our dataset. 


We want to study how the qualitative variable wages depends on one or more explanatory variables. Which
assumptions do we need make about the data if we want to perform a multiple linear regression analysis?

If we want to perform a MLR analysis, several assumptions need to be made. We assume a linear relationship between the response `Wage` and the covariates

$$ y_i = \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik} + \epsilon_i $$
Also, the errors $\epsilon_i$ are assumed to be independent, identically distributed with mean $0$ and constant variance $\sigma^2$. If we wish to construct confidence intervals and compute test statistics we must also assume normality in the errors $\epsilon_i \sim N(0, \sigma^2)$
SPØR METTE OM ANTAGELSENE KOMMER I RIKTIG REKKEFØLGE HER

TASK 2a
We develop our `mylm` to estimate the coefficients using the least squares estimator
$$ \hat{\beta} = (X^T X)^{-1} X^T Y $$
And confirm that this results in the same coefficient estimates as the included `lm` function
```{r}
library(mylm)
model1 = mylm(wages ~ education, data=SLID)
print(model1)

model2 = lm(wages ~education, data=SLID)
print(model2)
```

TASK 2b

INSERT HATS HERE

As $\beta = (X^T X)^{-1} X^T Y$ is a linear transformation of $Y$ and $Cov(Y) = \sigma^2 I$, 
$$Cov(\beta) = (X^T X)^{-1} X^T Cov(Y) X (X^T X)^{-1} = (X^T X)^{-1} \sigma^2$$
As we have also assumed that $Y \sim N(\beta X, \sigma^2)$, we get that
$$ \hat{\beta} \sim N(\beta, (X^T X)^{-1} \sigma^2) $$

We can then test the significance of the coefficients by computing the test statistics 
$$ z_j = \frac{(\beta_j - 0)}{ (c_{jj} \sigma^2)}, \quad c_{jj} = Cov(\beta)_{jj}$$
And the corresponding p-values 
$$ p_j = 2 Pr(Z \geq  | z_j |) $$
Note that we in `mylm` assume asymptotic results ($n - p$ very large) and we can then use the approximation $\hat{\sigma}^2 = \sigma^2$ without using the t-distribution.

Task2c

In `mylm` we implement the plot function as a comparison of the fitted values and the residuals. This should show no structure
```{r}
plot(model1)
```


Task2d

The residual sum of squares `SSE` is the sum of squares  $\sum^n (y_i - \hat{y}_i)^2$. The degrees of freedom for this model is the number of observations minus the number of fitted parameters, $n - p$. The total sum of squares similarly defined as $\sum^n (y_i - \bar{y})^2$. Under $H_0$, i.e. the response is uncorrelated with all covariates, 
$$\frac{(SST - SSE) / (p-1)}{SSE / (n - p)} \sim F_{p-1, n-p}$$
Again we assume that $(n - p) \rightarrow \infty$ results and get 
$$\frac{(SST - SSE)}{SSE / (n-p)} \sim \chi^2_{p-1}$$
Which we can use to test the significance of the total regression. 
```{r}
cat("Chi-square test statistic: ", model1$F_statistic, "\n")
cat("p-val: ", model1$F_p_val)
```
In a simple linear regression, $p = 2$ and the $z$-statistic and $\chi^2$-statistic becomes equivalent measures. THIS CAN BE SHOWN ANALYTICALLY. We confirm this by listing the critical $Z$-values of some some quantiles along with square of the $\chi^2$-values for the same quantiles. Note that the normal distributed quantiles are two-sided wheres the $\chi^2$ quantiles are one-sided. 
```{r}
interval = c(0.70, 0.85, 0.90, 0.95)
interval2 = c(0.40, 0.60, 0.80, 0.90)
cat(abs(qnorm(interval)), "\n")
cat(sqrt(qchisq(interval2, 1)))
```

Task2e

The coefficient of determination $R^2$ is the proportion of variance explained by the regression.  It is the square of the sample correlation between $y$ and $\hat{Y}$. 
$$ R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$

Task3a

```{r}
model2 = mylm(wages ~ education + age, data = SLID)
summary(model2)
```

Task3b

INTERPRET THE PARAMETERS HERE

```{r}
model20 = mylm(wages ~ education, data=SLID)
model21 = mylm(wages ~ age, data=SLID)
summary(model20)
summary(model21)
```

ANSWER THIS: Why and when does the parameter estimates found (using two simple and one multiple) differ? 

Task4

```{r}
model30 = mylm(wages ~ sex + age + language + I(education^2), data=SLID)
summary(model30)
plot(model30)
```

```{r}
model31 = mylm(wages ~ language + education + language*education, data=SLID)
summary(model31)
plot(model31)
```

```{r}
model32 = mylm(wages ~ education - 1, data=SLID)
summary(model32)
plot(model32)
```










