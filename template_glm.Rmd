--- 
title: 'TMA4315: Compulsory exercise 1 Linear models for Gaussian data'
output:
  pdf_document:
    toc: yes
    toc_depth: '2'
  html_document:
    toc: yes
    toc_depth: 2
    toc_float: yes
date: "`r format(Sys.time(), '%d.%m.%Y')`"
subtitle: 'Group 0: Henrik Syversveen Lie, Mikal Stapnes, Oliver Byhring'
---

```{r setup, include = FALSE}
library(ggplot2)
library(formatR)
showsol <- FALSE
library(knitr)
opts_chunk$set(tidy.opts = list(width.cutoff = 68), tidy = TRUE, warning = FALSE, error = FALSE, message = FALSE, echo = TRUE)
```

# Part 1: Explanatory analysis of the dataset

The following is a matrix of diagnostic plots of all the variables in the dataset from the `car` library.

```{r, eval=T, echo=FALSE}
installed.packages("car")
library(car)
library(GGally)
data(SLID, package = "carData")
SLID = SLID[complete.cases(SLID), ]
ggpairs(SLID)
```

From the diagnostic plots, we see that `Wage` has a positive correlation with the quantitative variables `education` and `age`. In addition we see that `sex`="Male" has a positive effect on `Wage`. Increasing `education`, `age` or switching `sex` to "Male" will, on average, result in an increase in `Wage` in our dataset. 

If we want to perform a MLR analysis, several assumptions need to be made. We assume a linear relationship between the response `Wage` and the covariates

$$ y_i = \beta_0 + \beta_1 x_{i1} + ... + \beta_k x_{ik} + \epsilon_i.$$
The errors $\epsilon_i$ are assumed to be independent, identically distributed with mean $0$ and constant variance $\sigma^2$. If we wish to construct confidence intervals and compute test statistics we must also assume normality in the errors $\epsilon_i \sim N(0, \sigma^2)$.

# Part 2: Simple linear regression with the `mylm` package

## a)

We develop our `mylm` to estimate the coefficients using the least squares estimator
$$ \hat{\beta} = (X^T X)^{-1} X^T Y.$$
We confirm that this results in the same coefficient estimates as the included `lm` function.
```{r}
library(mylm)
model1 = mylm(wages ~ education, data=SLID)
print(model1)

model2 = lm(wages ~education, data=SLID)
print(model2)
```

We also note that in the case of MLR, the maximum likelihood estimator is the same as the least squares estimator. 

## b)

As $\hat{\beta} = (X^T X)^{-1} X^T Y$ is a linear transformation of $Y$ and it is assumed that $\text{Cov}(Y) = \sigma^2 I$, we get that
$$\text{Cov}(\hat{\beta}) = (X^T X)^{-1} X^T \text{Cov}(Y) X (X^T X)^{-1} = (X^T X)^{-1} \sigma^2.$$
As we have also assumed that $Y \sim N(X \beta, \sigma^2)$, we get that
$$ \hat{\beta} \sim N(\beta, (X^T X)^{-1} \sigma^2) $$

We can then conduct a hypothesis test with hypotheses
$$H_0: \beta_j = 0, \quad H_1: \beta_j \neq 0.$$ 
We test the hypothesis using the test statistics
$$ z_j = \frac{(\beta_j - 0)}{ \sqrt{c_{jj}} \sigma}, \quad c_{jj} = (X^T X)^{-1}_{jj},$$
and the corresponding p-values 
$$ p_j = 2 \Pr(Z \geq  | z_j |) $$
Note that we in `mylm` assume asymptotic results ($n - p$ very large), which gives
$$ z_j \sim N(0, 1).$$
If we did not assume asymptotic results we would have that
$$ z_j \sim T_{n-p}$$

```{r, echo=F}
summary(model1)
```

In our model we get the coefficient values $\hat{\beta_0} = 4.97$ and $\hat{\beta_1} = 0.79$ with the corresponding estimated standard errors $\widehat{SE}(\beta_0) = 0.53$ and $\widehat{SE}(\beta_1) = 0.039$. This gives us test statistics $z_0 = 9.30$ and $z_1 = 20.28$, which are both highly significant under our model assumptions.

The coefficients can be interpreted as following: increasing `education` by one unit will increase the model response $\hat{Y}$ by $0.79$ units. The `(Intercept)` is the expected mean value of the response, when `education = 0`. If our model assumptions are correct and our data is a sufficiently good representation of the remaining population we would also, on average, expect the same effects of the covariates on the real response $Y$, `wages`.

## c)

In `mylm` we implement the plot function with the fitted values on the $x$-axis and the residuals on the $y$-axis.
```{r}
plot(model1)
```
From the plot we see that for lower values the residuals are mostly positive, whereas for higher values they are centered at $0$. This is expected, as $Y$ cannot be lower than $0$ (negative wages does not make sense). The variance also seems to increase with higher fitted values, meaning that there is some violation of the homoscedasticity of the errors. At higher fitted values, the density has a larger left tail (towards higher values) than right tail, which is in violation with the normality assumption of the errors. These violations indicate that our model assumptions are not entirely reasonable and we should proceed with caution when conducting inference. 

## d)

The residual sum of squares `SSE` is the sum of squares  $\sum^n_{i=1} (y_i - \hat{y}_i)^2$. The degrees of freedom for this model is the number of observations minus the number of fitted parameters, $n - p$. The total sum of squares `SST` is similarly defined as $\sum^n_{i=1} (y_i - \bar{y})^2$. The regression sum of squares is the difference $SST - SSE$. Under $H_0$ from b) we have that
$$\frac{(SST - SSE) / (p-1)}{SSE / (n - p)} \sim F_{p-1, n-p}.$$
Again we assume asymptotic results, i.e. that $(n - p) \rightarrow \infty$, and get 
$$\frac{(SST - SSE)}{SSE / (n-p)} \sim \chi^2_{p-1}.$$
Which we can use to test the significance of the total regression. 
```{r, echo = F, eval = T}
cat("Chi-square test statistic: ", model1$F_statistic, "\n")
cat("p-val: ", model1$F_p_val)
```
Because the p-value is below any reasonable significance level, we say that the regression is significant.

In simple linear regression, $p = 2$ and the $z$-test and $\chi^2$-test become equivalent tests of the same $H_0$. This comes as a result of the definition of a $\chi^2$ variable, which is that if $Z_1, Z_2, \dots, Z_k \sim N(0, 1)$, we have that the sum of their squares, 
$$ Q = \sum_{i=1}^{k}Z_i^2,$$
is distributed according to the $\chi^2$ distribution with $k$ degrees of freedom, denoted as $Q \sim \chi^2_k$. Seeing as the $z$-statistic is standard normal distributed, its square $z^2$ will be $\chi^2$ distributed with $1$ degree or freedom, $z^2 \sim \chi^2_1$. This means that the $z$- and $\chi^2$ statistics will reject the null hypothesis at the same levels of significance.

We confirm this by listing the critical $Z$-values of some some quantiles along with square of the $\chi^2$-values for the same quantiles. Note that the normal distributed quantiles are two-sided wheres the $\chi^2$ quantiles are one-sided. 
```{r}
interval = c(0.70, 0.8, 0.90, 0.95)
interval2 = c(0.40, 0.60, 0.80, 0.90)
cat(abs(qnorm(interval)), "\n")
cat(sqrt(qchisq(interval2, 1)))
```

## e)

The coefficient of determination, $R^2$, is the square of the sample correlation between $y$ and $\hat{Y}$. It is a measure of the proportion of variance explained by the regression. 
$$ R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$
We observe from the summary that in our model we get $R^2 = 0.09$, which indicates that we are not able to explain a large amount of the variance in the data using our model. This might indicate that the response we are trying to model, `wages`, has high variability, or that our model is too simple capture the true distribution. In this example we suspect it to be a combination of both. 

# Part 3: Multiple linear regression

## a)
So far we have implemented the our `mylm`to handle both simple and multivariate linear regression. For implementation details, see Part 1. 
```{r}
model3 = mylm(wages ~ education + age, data = SLID)
```

## b)
```{r}
summary(model3)
```
In our model we get the coefficient values $\hat{\beta_0} = -6.02$, $\hat{\beta}_{edu} = 0.902$, $\hat{\beta}_{age} = 0.257$ with the corresponding estimated standard errors $\widehat{SE}(\beta_0) = 0.619$, $\widehat{SE}(\beta_{edu}) = 0.036$ and $\widehat{SE}(\beta_{age}) = 0.257$. This gives us test statistics $z_0 = -9.73$, $z_1 = 25.2$ and $z_2 = 28.7$, which are all highly significant under model assumptions. We get a $\chi^2$-statistic of $661$ on $2$ degrees of freedom, which is indicates that the total regression is also highly significant. 

The coefficients can be interpreted as following: increasing only `education` by one unit will increase the model response $\hat{Y}$ by $0.902$ units. Increasing only `age` by one unit will increase the model response $\hat{Y}$ by $0.257$ units. The `(Intercept)` is the mean of the responses, and is the model response in the case that `education` $ = 0$ and `age` $=0$. If our model assumptions are correct and our data is a sufficiently good representation of the remaining population we would also, on average, expect the same effects of the covariates on the real response $Y$, `wages`.

## c)
```{r}
model30 = mylm(wages ~ education, data=SLID)
model31 = mylm(wages ~ age, data=SLID)
summary(model30)
summary(model31)
```

The parameter estimates for the simple and multiple regression differ if the covariates are correlated. Then, in the case of the simple regression, the single explanatory variable pick up some of the variance that would otherwise be explained by the additional explanatory variable. The $(p, p)$ matrix $(X^T X)$ is
$$ \begin{pmatrix} x_0^T x_0 & x_0^T x_1 & x_0^T x_2 \\
x_1^T x_0 & x_1^T x_1 & x_1^T x_2 \\
x_2^T x_0 & x_2^T x_1 & x_2^T x_2 \\ 
\end{pmatrix},$$
If we have uncorrelated covariates, we get that $x_i^T x_j = 0 \quad \forall i \neq j$, such that $(X^T X)$ becomes
$$ \begin{pmatrix} x_0^T x_0 & 0 & 0 \\
0 & x_1^T x_1 & 0 \\
0 & 0 & x_2^T x_2 \\ 
\end{pmatrix},$$
which is diagonal and thus each coeffecient estimate $\hat{\beta}_j = (x_j^T x_j)^{-1} x_j^T y_j$ is independent of the other covariates. 

In our case we get different estimates in the simple and mupltiple case, which is reasonable as `education` and `age` have a weak positive correlation. 
```{r}
model3$corr_coeff
```

# Part 4: Testing the `mylm`package


```{r}
model40 = mylm(wages ~ sex + age + language + I(education^2), data=SLID)
summary(model40)
plot(model40)
```

The difference between this model and earlier MLR is that we are now a squared term, $'education'^2$. Our linear model assumption is now,
$$ Y_i = \beta_0 + \beta_{sex} x_{i, sex} + \beta_{age} x_{i, age} + \beta_{lan} x_{i, lan} + \beta_{edu^2} x_{i, edu}^2 + \epsilon_i.$$
Fitting our model, we see positive terms for the covariates `sex`, `age` and `education$^2$` and negative for both levels of `language`. 

The coefficient $\hat{\beta}_{edu^2}$ can be interpreted as the following: increasing the square of `education` by one unit will increase the model response, $\hat{Y}$, by $0.035$ units. We also notet that covariates that are fitted to a negative coefficient will produce a decrease in the model response $\hat{Y}$. All remaining coefficients can be interpreted as in 3b. 

We notice that all variables attain a high level of significance with the exception of language. Thus, to improve the model, language should be removed, as even under model assumptions it is probable that it is uncorrelated with the response. The $\chi^2$-statistic of $345$ on $5$ degrees of freedom indicates that the total regression is highly significant. 

The error plot shows a lower bound of the residuals in the left corner. Again the explanation is that we cannot have negative `wages`. Beyond this, we observe some level of homoscedasticity and normality in our errors, which indicates that our model assumptions are not entirely reasonable. 

```{r}
model41 = mylm(wages ~ language + education + language*education, data=SLID)
summary(model41)
plot(model41)
```

Now we have included an interaction term. Our linear model assumption is now,

$$ Y_i = \beta_0 + \beta_{edu} x_{i, edu} + \beta_{lan} x_{i, lan} + \beta_{edu \& lan} x_{i, edu} x_{i, lan} + \epsilon_i.$$
We interpret the interaction term between `language` and `education` in the following way. When the level of language increases from 0 to 1 (from `English` to `French`), the response is incresed by the coefficient estimate for `French` plus an interaction term, which is the coefficient estimate for the interaction multiplied by the `education` covariate,
$$Y_{increase} = \beta_{languageFrench} + \beta_{edu\&lan} x_{edu}.$$

A $\chi^2$-statistic with a corresponding p-value $<2.2 \times 10^{-16}$ signifies that the total regression is significant. The intercept, `languageOther` and `education` are all significant on a $0.001$ level. The interaction between `languageOther`and `education` is significant on a $0.01$ level, `languageFrench` on a $0.05$ level and the interaction between `languageFrench` and `education` on a $0.1$ level. 

Seeing as `languageFrench` and `languageOther` have roughly the same coefficient estimate, one could use the natural assumption that they have the same effect and merge these two levels into one, giving `language` only two levels. 

In addition, a multiple R-squared of $0.1$ can be interpreted as the model explaining 10% of the variance, this is not a high number, and the model could be improved by for example adding another covariate, e.g. `sex` or `age`. 

The error plot shows the same lower bound as previously discussed. We observe a trend of increasing variance with higher fitted values. For lower fitted values we see some off-centering but around a relatively small amount of points. Again we observe a larger left tail. There is significant violation of both the centering, homoscedasticity and normality of the errors. 


```{r}
model42 <- mylm(wages ~ education - 1, data=SLID)
summary(model42)
plot(model42)
```



Now we have a model without intercept. Our linear model assumption is now
$$ Y_i = \beta_{edu} x_{i, edu} + \epsilon_i.$$

The interpretation of this model is that `wage` is only a scaling of `education`, with some individual differences $\epsilon_i$. From a realistic point of view, this model does not make sense; one would not expect zero income with zero education, as some jobs does not require education at all. Thus, the model would be improved if we add intercept.

The coefficient estimate of `education` is positive and significant at a $0.001$ level, which is as expected because higher education usually equates to higher income.

The error plot strongly resembles that of the previous example and induces the same conclusions. 



